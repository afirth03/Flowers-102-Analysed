{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afirth03/IMLOAssessment/blob/main/IMLOExam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp_L3NW6BIjU"
      },
      "source": [
        "# Flowers 102 Dataset Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvrJRT3kDg3L"
      },
      "source": [
        "Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4WPcjT1p_odR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "outputId": "cd0176eb-0e5e-4b7b-c66d-187634ed8200"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/__init__.py:1595\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hub \u001b[38;5;28;01mas\u001b[39;00m hub\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m random \u001b[38;5;28;01mas\u001b[39;00m random\n\u001b[0;32m-> 1595\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distributions \u001b[38;5;28;01mas\u001b[39;00m distributions\n\u001b[1;32m   1596\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m testing \u001b[38;5;28;01mas\u001b[39;00m testing\n\u001b[1;32m   1597\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backends \u001b[38;5;28;01mas\u001b[39;00m backends\n",
            "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/distributions/__init__.py:110\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpoisson\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Poisson\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrelaxed_bernoulli\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RelaxedBernoulli\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrelaxed_categorical\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RelaxedOneHotCategorical\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstudentT\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StudentT\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransformed_distribution\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformedDistribution\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap_external>:846\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap_external>:941\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "File \u001b[0;32m<frozen importlib._bootstrap_external>:1039\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation = 0.2, hue = 0.1),\n",
        "    transforms.RandomAffine(degrees = 0, translate = (0.1,0.1), scale=(0.9, 1.1), shear = 10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomErasing(p=0.1, scale = (0.01, 0.1), ratio=(0.9, 1.1)),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_data = datasets.Flowers102(\n",
        "    root=\"data\",\n",
        "    split=\"train\",\n",
        "    download=True,\n",
        "    transform=transform_train\n",
        ")\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "\n",
        "test_data = datasets.Flowers102(\n",
        "    root=\"data\",\n",
        "    split=\"test\",\n",
        "    download=True,\n",
        "    transform=transform\n",
        "\n",
        ")\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=0)\n",
        "\n",
        "val_data = datasets.Flowers102(\n",
        "    root=\"data\",\n",
        "    split=\"val\",\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "valloader = torch.utils.data.DataLoader(val_data, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=0)\n",
        "\n",
        "classes = {\"0\": \"error\", \"21\": \"fire lily\", \"3\": \"canterbury bells\", \"45\": \"bolero deep blue\", \"1\": \"pink primrose\", \"34\": \"mexican aster\", \"27\": \"prince of wales feathers\", \"7\": \"moon orchid\", \"16\": \"globe-flower\", \"25\": \"grape hyacinth\", \"26\": \"corn poppy\", \"79\": \"toad lily\", \"39\": \"siam tulip\", \"24\": \"red ginger\", \"67\": \"spring crocus\", \"35\": \"alpine sea holly\", \"32\": \"garden phlox\", \"10\": \"globe thistle\", \"6\": \"tiger lily\", \"93\": \"ball moss\", \"33\": \"love in the mist\", \"9\": \"monkshood\", \"102\": \"blackberry lily\", \"14\": \"spear thistle\", \"19\": \"balloon flower\", \"100\": \"blanket flower\", \"13\": \"king protea\", \"49\": \"oxeye daisy\", \"15\": \"yellow iris\", \"61\": \"cautleya spicata\", \"31\": \"carnation\", \"64\": \"silverbush\", \"68\": \"bearded iris\", \"63\": \"black-eyed susan\", \"69\": \"windflower\", \"62\": \"japanese anemone\", \"20\": \"giant white arum lily\", \"38\": \"great masterwort\", \"4\": \"sweet pea\", \"86\": \"tree mallow\", \"101\": \"trumpet creeper\", \"42\": \"daffodil\", \"22\": \"pincushion flower\", \"2\": \"hard-leaved pocket orchid\", \"54\": \"sunflower\", \"66\": \"osteospermum\", \"70\": \"tree poppy\", \"85\": \"desert-rose\", \"99\": \"bromelia\", \"87\": \"magnolia\", \"5\": \"english marigold\", \"92\": \"bee balm\", \"28\": \"stemless gentian\", \"97\": \"mallow\", \"57\": \"gaura\", \"40\": \"lenten rose\", \"47\": \"marigold\", \"59\": \"orange dahlia\", \"48\": \"buttercup\", \"55\": \"pelargonium\", \"36\": \"ruby-lipped cattleya\", \"91\": \"hippeastrum\", \"29\": \"artichoke\", \"71\": \"gazania\", \"90\": \"canna lily\", \"18\": \"peruvian lily\", \"98\": \"mexican petunia\", \"8\": \"bird of paradise\", \"30\": \"sweet william\", \"17\": \"purple coneflower\", \"52\": \"wild pansy\", \"84\": \"columbine\", \"12\": \"colt's foot\", \"11\": \"snapdragon\", \"96\": \"camellia\", \"23\": \"fritillary\", \"50\": \"common dandelion\", \"44\": \"poinsettia\", \"53\": \"primula\", \"72\": \"azalea\", \"65\": \"californian poppy\", \"80\": \"anthurium\", \"76\": \"morning glory\", \"37\": \"cape flower\", \"56\": \"bishop of llandaff\", \"60\": \"pink-yellow dahlia\", \"82\": \"clematis\", \"58\": \"geranium\", \"75\": \"thorn apple\", \"41\": \"barbeton daisy\", \"95\": \"bougainvillea\", \"43\": \"sword lily\", \"83\": \"hibiscus\", \"78\": \"lotus lotus\", \"88\": \"cyclamen\", \"94\": \"foxglove\", \"81\": \"frangipani\", \"74\": \"rose\", \"89\": \"watercress\", \"73\": \"water lily\", \"46\": \"wallflower\", \"77\": \"passion flower\", \"51\": \"petunia\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YK2Yqr16M3j"
      },
      "source": [
        "##Matching the label to the flower name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "JIkISH_dMFRM"
      },
      "outputs": [],
      "source": [
        "def flower_name(label, classes):\n",
        "    flower_name = {int(k): v for k, v in classes.items()}\n",
        "    return flower_name.get(label, \"Unknown\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5D8f9SRRVMh"
      },
      "source": [
        "# Display Processed images randomly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nR4NCzrPJKbS"
      },
      "outputs": [],
      "source": [
        "figure = plt.figure(figsize=(15, 10))\n",
        "cols, rows = 3, 2\n",
        "\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(val_data), size=(1,)).item()\n",
        "    img, label = train_data[sample_idx]\n",
        "    img = img.permute(1, 2, 0)\n",
        "    ax = figure.add_subplot(rows, cols, i)\n",
        "    ax.set_title(f'Label: {label} Flower: {flower_name(label, classes)}')\n",
        "    ax.axis(\"on\")\n",
        "    ax.imshow(img.squeeze(), cmap=\"Accent\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXS3q5Y81lOF"
      },
      "source": [
        "# Creating Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxpahQ0xt5lQ"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential( #Sequential for modularity\n",
        "\n",
        "            nn.Conv2d(3, 64, 5, stride = 2, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 128, 5, stride = 2, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(5, 2),\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, stride = 1, padding = 1), #Reduced stride from 2 to 1 to capture detailed info from images\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, stride = 1, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.MaxPool2d(5, 2),\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, stride = 1, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, stride = 1, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.MaxPool2d(5, 2),\n",
        "\n",
        "            nn.AdaptiveAvgPool2d((14, 14)) #Makes the input fit a 512 * 14 * 14 input\n",
        "\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 14 * 14, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Dropout(0.5), #Dropout data to reduce overfitting\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(512, 102)  #Output classes in Flowers 102\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)  #Flatten the output for the fully connected layer\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "cnn = CNN()\n",
        "print(cnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He8JLI5g6GqM"
      },
      "source": [
        "##Training and testing the CNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cnn.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(cnn.parameters(), lr=0.001, momentum=0.9,weight_decay=0.0001)\n",
        "cnn.train()\n",
        "\n",
        "for epoch in range(200):\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0], data[1]\n",
        "\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = cnn(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        average_train_loss = running_loss / len(trainloader)\n",
        "\n",
        "        if i % 8 == 7:\n",
        "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 8:.3f}')\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "id": "WM2hjIBflr8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.eval()\n",
        "running_loss = 0.0\n",
        "running_accuracy = 0.0\n",
        "\n",
        "\n",
        "for i, data in enumerate(valloader):\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = cnn(inputs)\n",
        "        correct = torch.sum(labels == torch.argmax(outputs, dim=1)).item()\n",
        "        running_accuracy += correct / batch_size\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "avg_loss_across_batches = running_loss / len(valloader)\n",
        "avg_acc_across_batches = (running_accuracy / len(valloader)) * 100\n",
        "\n",
        "print(f'Val Loss: {avg_loss_across_batches:.3f}, Val Accuracy: {avg_acc_across_batches:.1f}%')"
      ],
      "metadata": {
        "id": "ippUel5IlvOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = cnn(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the model on the test images: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "jeXaw3AtlxqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(cnn.state_dict(), 'cnn_flowers102.pth')"
      ],
      "metadata": {
        "id": "Bh3XnqJYAeE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# **FINAL TESTING RESULTS**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DNI4adPJNM5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(30),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation = 0.2, hue = 0.1),\n",
        "    transforms.RandomAffine(degrees = 0, translate = (0.1,0.1), scale=(0.9, 1.1), shear = 10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomErasing(p=0.1, scale = (0.01, 0.1), ratio=(0.9, 1.1)),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_data = datasets.Flowers102(\n",
        "    root=\"data\",\n",
        "    split=\"train\",\n",
        "    download=True,\n",
        "    transform=transform_train\n",
        ")\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "\n",
        "test_data = datasets.Flowers102(\n",
        "    root=\"data\",\n",
        "    split=\"test\",\n",
        "    download=True,\n",
        "    transform=transform\n",
        "\n",
        ")\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=0)\n",
        "\n",
        "val_data = datasets.Flowers102(\n",
        "    root=\"data\",\n",
        "    split=\"val\",\n",
        "    download=True,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "valloader = torch.utils.data.DataLoader(val_data, batch_size=batch_size,\n",
        "                                          shuffle=False, num_workers=0)\n",
        "\n",
        "classes = {\"0\": \"error\", \"21\": \"fire lily\", \"3\": \"canterbury bells\", \"45\": \"bolero deep blue\", \"1\": \"pink primrose\", \"34\": \"mexican aster\", \"27\": \"prince of wales feathers\", \"7\": \"moon orchid\", \"16\": \"globe-flower\", \"25\": \"grape hyacinth\", \"26\": \"corn poppy\", \"79\": \"toad lily\", \"39\": \"siam tulip\", \"24\": \"red ginger\", \"67\": \"spring crocus\", \"35\": \"alpine sea holly\", \"32\": \"garden phlox\", \"10\": \"globe thistle\", \"6\": \"tiger lily\", \"93\": \"ball moss\", \"33\": \"love in the mist\", \"9\": \"monkshood\", \"102\": \"blackberry lily\", \"14\": \"spear thistle\", \"19\": \"balloon flower\", \"100\": \"blanket flower\", \"13\": \"king protea\", \"49\": \"oxeye daisy\", \"15\": \"yellow iris\", \"61\": \"cautleya spicata\", \"31\": \"carnation\", \"64\": \"silverbush\", \"68\": \"bearded iris\", \"63\": \"black-eyed susan\", \"69\": \"windflower\", \"62\": \"japanese anemone\", \"20\": \"giant white arum lily\", \"38\": \"great masterwort\", \"4\": \"sweet pea\", \"86\": \"tree mallow\", \"101\": \"trumpet creeper\", \"42\": \"daffodil\", \"22\": \"pincushion flower\", \"2\": \"hard-leaved pocket orchid\", \"54\": \"sunflower\", \"66\": \"osteospermum\", \"70\": \"tree poppy\", \"85\": \"desert-rose\", \"99\": \"bromelia\", \"87\": \"magnolia\", \"5\": \"english marigold\", \"92\": \"bee balm\", \"28\": \"stemless gentian\", \"97\": \"mallow\", \"57\": \"gaura\", \"40\": \"lenten rose\", \"47\": \"marigold\", \"59\": \"orange dahlia\", \"48\": \"buttercup\", \"55\": \"pelargonium\", \"36\": \"ruby-lipped cattleya\", \"91\": \"hippeastrum\", \"29\": \"artichoke\", \"71\": \"gazania\", \"90\": \"canna lily\", \"18\": \"peruvian lily\", \"98\": \"mexican petunia\", \"8\": \"bird of paradise\", \"30\": \"sweet william\", \"17\": \"purple coneflower\", \"52\": \"wild pansy\", \"84\": \"columbine\", \"12\": \"colt's foot\", \"11\": \"snapdragon\", \"96\": \"camellia\", \"23\": \"fritillary\", \"50\": \"common dandelion\", \"44\": \"poinsettia\", \"53\": \"primula\", \"72\": \"azalea\", \"65\": \"californian poppy\", \"80\": \"anthurium\", \"76\": \"morning glory\", \"37\": \"cape flower\", \"56\": \"bishop of llandaff\", \"60\": \"pink-yellow dahlia\", \"82\": \"clematis\", \"58\": \"geranium\", \"75\": \"thorn apple\", \"41\": \"barbeton daisy\", \"95\": \"bougainvillea\", \"43\": \"sword lily\", \"83\": \"hibiscus\", \"78\": \"lotus lotus\", \"88\": \"cyclamen\", \"94\": \"foxglove\", \"81\": \"frangipani\", \"74\": \"rose\", \"89\": \"watercress\", \"73\": \"water lily\", \"46\": \"wallflower\", \"77\": \"passion flower\", \"51\": \"petunia\"}"
      ],
      "metadata": {
        "id": "C7GvPrg-glVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential( #Sequential for modularity\n",
        "\n",
        "            nn.Conv2d(3, 64, 5, stride = 2, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Conv2d(64, 128, 5, stride = 2, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.MaxPool2d(5, 2),\n",
        "\n",
        "            nn.Conv2d(128, 256, 3, stride = 1, padding = 1), #Reduced stride from 2 to 1 to capture detailed info from images\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, 3, stride = 1, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.MaxPool2d(5, 2),\n",
        "\n",
        "            nn.Conv2d(256, 512, 3, stride = 1, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.Conv2d(512, 512, 3, stride = 1, padding = 1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.MaxPool2d(5, 2),\n",
        "\n",
        "            nn.AdaptiveAvgPool2d((14, 14)) #Makes the input fit a 512 * 14 * 14 input\n",
        "\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * 14 * 14, 1024),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(1024),\n",
        "            nn.Dropout(0.5), #Dropout data to reduce overfitting\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm1d(512),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(512, 102)  #Output classes in Flowers 102\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)  #Flatten the output for the fully connected layer\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "cnn = CNN()\n",
        "print(cnn)"
      ],
      "metadata": {
        "id": "ALeKI8fqZpP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "cnn.to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "_sLMV33LZ-XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = CNN()\n",
        "cnn.load_state_dict(torch.load('cnn_flowers102.pth', map_location=device))"
      ],
      "metadata": {
        "id": "XtCIMIv_Yx3v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.eval()\n",
        "running_loss = 0.0\n",
        "running_accuracy = 0.0\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for i, data in enumerate(valloader):\n",
        "    inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = cnn(inputs)\n",
        "        correct = torch.sum(labels == torch.argmax(outputs, dim=1)).item()\n",
        "        running_accuracy += correct / batch_size\n",
        "        loss = criterion(outputs, labels)\n",
        "        running_loss += loss.item()\n",
        "\n",
        "avg_loss_across_batches = running_loss / len(valloader)\n",
        "avg_acc_across_batches = (running_accuracy / len(valloader)) * 100\n",
        "\n",
        "print(f'Val Loss: {avg_loss_across_batches:.3f}, Val Accuracy: {avg_acc_across_batches:.1f}%')"
      ],
      "metadata": {
        "id": "3kX_azMwNOyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in testloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = cnn(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Accuracy of the model on the test images: {accuracy:.2f}%')"
      ],
      "metadata": {
        "id": "pR7DVjp3NSPd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOP8tr51+wkb2v/KM7yMPs+",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}